---
title: "Classification II"
---

## Summary

This week built on last week's classification session, but took a different turn by focusing on pre-classified datasets and sub-pixel analysis. Instead of starting from raw satellite imagery, we explored ready-to-use classification products like GlobeLand30, MODIS, and Dynamic World. These datasets allow us to work faster but raise questions about the loss of control over classification accuracy and assumptions baked into the processing.

The key focus for me was sub-pixel classification—a method that tries to identify multiple land cover types within a single pixel. This is really helpful when working with coarse resolution imagery where, for example, a 30m pixel might contain buildings, roads, and trees. Traditional classification would assign it one label, but sub-pixel methods estimate the proportions of each type instead.

![Fig. 1 A zoomed-in pixel showing buildings, vegetation, and roads (MacLachlan et al., 2017)](week_8/zoomed_in_pixel.png)

Sub-pixel analysis is especially useful in urban fringe areas, where land cover is complex and mixed. But it also raises new challenges around validation and reliability. That leads us into the importance of accuracy assessment.

### Accuracy Assessment

In remote sensing, three key metrics help evaluate performance:

Producer Accuracy – How well the model captures a class based on reference data ![](week_8/Picture8.png){width="30" height="11"}

User Accuracy – How likely the modelled class is correct when visited on the ground![](week_8/Picture9.png){width="30" height="12"}

Overall Accuracy – The total correct classifications across all classes ![](week_8/Picture10.png){width="30" height="12"}

![Fig. 2 Confusion matrix from Dynamic World (Brown et al., 2022)](week_8/Picture6.png)

Looking at Dynamic World, for instance, the model achieved 94% recall for water; meaning it detected most water pixels—but only 87% user accuracy; meaning some areas it labelled as water were not actually water on the ground. Averaging across all classes, the overall accuracy came to around 72%, which reflects the trade-off between global coverage and accuracy.

There is another metric called the Kappa coefficient, which accounts for the agreement between the model and random chance. It's a more robust measure than overall accuracy, especially when classes are imbalanced.

## Applications

For this week, I focus what methods they use for their sub pixel classification and how they validate their results. And reflect on trade-off between accuracy and coverage.

The first study is by Suresh & Jain (2018) explored subpixel classification on a small urban scale using high resolution QuickBird imagery (0.6m). They adopted two main methods, colorimetry and chromaticity diagrams.They used colorimetry, specifically the CIE-XYZ colour space, to model how colours within a pixel mix, estimating the proportions of different land cover types. Then, comparing chromatic distances between neighbouring pixels, they broke down mixed pixels more accurately. They validated their method using a Fuzzy Error Matrix, which showed better results than standard soft classifiers. This is shown the table below.

![Table. 1 Accuracy outputs comparing authors proposed and conventional approach (Suresh and Jain, 2018)](week_8/Picture11.png)

Insights

This is a unique method because it doesn't rely om just spectral band but rather looks at how colours mix inside a pixel. I am wondering method might be bias when it has pixels with in low surfaces reflectance (water bodies) or surfaces with very similar colours. With high resolution sensor, I understand why it was utilized to classify urban cities on a small scale but I am not sure how well it would hold up across a larger area with many variation of colours.

Also, I am curious on how it deals with shadows. Assessment test showed the method had a high accuracy in a urban setting, meaning the was able to classify even with the high presence of shadows. I think they try the method on a larger scale to see how well it performs as well.

## Reflection

This week is last but one week with an entry for the learning diary. I haven’t had the chance to dive deeply into all the methods and models introduced in class, but I’ve come to realise that delving into a method in depth helps build real proficiency. That said, there’s a trade-off focusing deeply on a few methods can mean having only a surface level understanding of others. I think my approach is to understand the purpose of each method and its limitations. This way, I can decide when a method is appropriate and even explore how to combine these methods for better results. This week, I chose a paper that introduced a method we hadn’t covered in class, just to broaden my scope. The idea of classifying a pixel based on how colours mix within it was fascinating. It’s a different angle from the typical spectral-band-driven approaches.

This week, I chose a paper that introduced a method we hadn’t covered in class, just to broaden my scope. The idea of classifying a pixel based on how colours mix within it was fascinating. It’s a different angle from the typical spectral-band-driven approaches.

## Reference

Brown, C.F., Brumby, S.P., Guzder-Williams, B., Birch, T., Hyde, S.B., Mazzariello, J., Czerwinski, W., Pasquarella, V.J., Haertel, R., Ilyushchenko, S., Schwehr, K., Weisse, M., Stolle, F., Hanson, C., Guinan, O., Moore, R., Tait, A.M., 2022. Dynamic World, Near real-time global 10 m land use land cover mapping. Sci. Data 9, 251. https://doi.org/10.1038/s41597-022-01307-4

MacLachlan, A., Roberts, G., Biggs, E., Boruff, B., 2017. Subpixel land-cover classification for improved urban area estimates using Landsat. Int. J. Remote Sens. 38, 5763–5792. https://doi.org/10.1080/01431161.2017.1346403

Suresh, M., Jain, K., 2018. Subpixel level mapping of remotely sensed image using colorimetry. Egypt. J. Remote Sens. Space Sci. 21, 65–72. https://doi.org/10.1016/j.ejrs.2017.02.004
